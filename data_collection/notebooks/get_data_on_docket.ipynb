{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect the comments on a single docket\n",
    "\n",
    "This notebook allows you to collect the data and metadata on all the comments on a single docket from the Regulations.gov API. It also extracts the text from comments submitted as PDFs and structure the data into a CSV file with the comments and metadata.\n",
    "\n",
    "We might already have collected the comments on the docket you are looking for. In that case, you can search for the docket on [www.commons-project.com/dockets](https://www.commons-project.com/dockets).\n",
    "\n",
    "It is also possible to download data in bulk via regulations.gov (see [here](https://www.regulations.gov/bulkdownload)). However, it can take up to 24 hours to receive the data, and the full text of comments uploaded as PDFs are not included in the bulk download."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define `docket_id`\n",
    "\n",
    "You can find the exact docket ID on [regulations.gov](https://www.regulations.gov/).\n",
    "\n",
    "The docket ID is the string of characters that comes after the last dash in the URL of the docket. For example, in the URL `https://www.regulations.gov/docket/EPA-HQ-OLEM-2023-0278`, the docket ID is `EPA-HQ-OLEM-2023-0278`. \n",
    "\n",
    "The docket ID is case-sensitive and has to match exactly the docket ID on regulations.gov for the API to return the correct data.\n",
    "\n",
    "Example:\n",
    "\n",
    "```bash\n",
    "docket_id = \"EPA-HQ-OLEM-2023-0278\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docket_id = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Load in the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import html\n",
    "import json\n",
    "from flatten_json import flatten\n",
    "import math\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import datetime as dt\n",
    "from datetime import date, datetime, timedelta\n",
    "from glob import glob\n",
    "from dotenv import load_dotenv\n",
    "from io import BytesIO\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "import pdfplumber\n",
    "import psycopg2\n",
    "import PyPDF2\n",
    "import pytz\n",
    "import requests\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTChar, LTTextContainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API keys\n",
    "\n",
    "Before you can run this notebook, you need to get an API key from regulations.gov. You can get one by going to https://open.gsa.gov/api/regulationsgov/ and clicking on the \"Get API Key\" button. Once you have the key, you can load it into the `.env` file.\n",
    "\n",
    "```bash\n",
    "echo \"REGULATIONS_GOV_API_KEY=your-key-here\" > .env\n",
    "```\n",
    "\n",
    "Load in the regulations.gov API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"REGGOV_API_KEY\")\n",
    "extra_api_key = os.getenv(\"REGGOV_API_KEY_N1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Collect the comments\n",
    "\n",
    "### 2.1 Get all the comment ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_ids = []\n",
    "for page in range(1, 20):\n",
    "    url = f\"https://api.regulations.gov/v4/comments?filter[docketId]={docket_id}&page[size]=250&page[number]={page}&sort=lastModifiedDate&api_key={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    result = response.json()\n",
    "    for item in result[\"data\"]:\n",
    "        comment_ids.append(item['id'])\n",
    "\n",
    "api_response = []\n",
    "\n",
    "if result[\"data\"] is None or not result[\"data\"]:\n",
    "    print(\"Seems like there are less than 250 comments in this docket.\")\n",
    "else:\n",
    "    api_response.append(result)\n",
    "\n",
    "# If there are more than 250 comments, we need to make additional API calls to get all the comments\n",
    "if result['meta']['totalElements'] > 250:\n",
    "\n",
    "    # Reset the time variables to get the last modified date of the last comment in the first API call. Thus we can use this date to get the next batch of comments\n",
    "    greater_than = api_response[-1][\"data\"][-1][\"attributes\"][\"lastModifiedDate\"][:-1]\n",
    "    greater_than = greater_than.replace(\"T\", \" \")\n",
    "    date_str = greater_than\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "    date_obj = datetime.strptime(date_str, date_format)\n",
    "    greater_than = date_obj - timedelta(\n",
    "        hours=5\n",
    "    )\n",
    "\n",
    "    for page in range(1, 20):\n",
    "        url = f\"https://api.regulations.gov/v4/comments?filter[docketId]={docket_id}&filter[lastModifiedDate][ge]={greater_than}&page[size]=250&page[number]={page}&sort=lastModifiedDate&api_key={api_key}\"\n",
    "        response = requests.get(url)\n",
    "        result = response.json()\n",
    "        for item in result[\"data\"]:\n",
    "            comment_ids.append(item['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Collected the IDs for {len(comment_ids)} comments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Get the metadata for each comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\"L1\", \"L2\", \"L3\", \"L4\", \"L5\", \"J1\", \"J2\", \"J3\", \"J4\", \"J5\", \"M1\", \"M2\", \"M3\", \"M4\", \"M5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out how many rounds it will take to scrape all the comments\n",
    "rounds = len(comment_ids) / 500\n",
    "rounds = math.ceil(rounds)\n",
    "\n",
    "comment_details = []\n",
    "\n",
    "# We scrape the comments using their index number in the ids list\n",
    "num = 0\n",
    "\n",
    "for round in range(rounds):\n",
    "    for key in keys:\n",
    "        api_key = os.getenv(f\"REGGOV_API_KEY_{key}\")\n",
    "        for i in range(50):\n",
    "            # In the last round, there might not be 50 comments left, so we break when we run out of comments to scrape\n",
    "            try:\n",
    "                comment_id = comment_ids[num]\n",
    "                docket_id = comment_id[:-5]\n",
    "                url = f\"https://api.regulations.gov/v4/comments/{comment_id}?include=attachments&api_key={api_key}\"\n",
    "                response = requests.get(url)\n",
    "                result = response.json()\n",
    "\n",
    "                # And append the data to the today_comments list so we can access the download links and store the pdfs\n",
    "                comment_details.append(result)\n",
    "            \n",
    "            except Exception as e:\n",
    "                    break\n",
    "\n",
    "            num = num + 1\n",
    "\n",
    "            # Sleep for 0.4 seconds to avoid hitting the API rate limit\n",
    "            time.sleep(0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Extract the text from comments stored as PDFs\n",
    "Extract the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comment_text(comments):\n",
    "\n",
    "    # Function to extract text\n",
    "    def text_extraction(element):\n",
    "        # Extracting the text from the in-line text element\n",
    "        line_text = element.get_text()\n",
    "\n",
    "        # Find the formats of the text\n",
    "        # Initialize the list with all the formats that appeared in the line of text\n",
    "        line_formats = []\n",
    "        for text_line in element:\n",
    "            if isinstance(text_line, LTTextContainer):\n",
    "                # Iterating through each character in the line of text\n",
    "                for character in text_line:\n",
    "                    if isinstance(character, LTChar):\n",
    "                        # Append the font name of the character\n",
    "                        line_formats.append(character.fontname)\n",
    "                        # Append the font size of the character\n",
    "                        line_formats.append(character.size)\n",
    "        # Find the unique font sizes and names in the line\n",
    "        format_per_line = list(set(line_formats))\n",
    "\n",
    "        # Return a tuple with the text in each line along with its format\n",
    "        return (line_text, format_per_line)\n",
    "\n",
    "    # Loop through the comments and extract the text from the attached pdfs\n",
    "    for comment in comments:\n",
    "        id = comment[\"data\"][\"id\"]\n",
    "        try:\n",
    "            num = 1\n",
    "            attachment_url = \"\"\n",
    "            for files in comment[\"included\"]:\n",
    "                result = \"\"\n",
    "                # Loop through the files and get the pdfs if they exist\n",
    "                if files[\"attributes\"][\"fileFormats\"] is not None:\n",
    "                    for file in files[\"attributes\"][\"fileFormats\"]:\n",
    "                        try:\n",
    "                            url = file[\"fileUrl\"]\n",
    "                            response = requests.get(url)\n",
    "                            attachment_url = attachment_url + str(url) + \" \"\n",
    "\n",
    "                            # pdf_path = f\"{id}_attachment_{num}.pdf\"\n",
    "                            pdf_path = os.path.abspath(f\"{id}_attachment_{num}.pdf\")\n",
    "                            doc_path = os.path.abspath(\"temp.docx\")\n",
    "                            soffice_path = (\n",
    "                                \"/Applications/LibreOffice.app/Contents/MacOS/soffice\"\n",
    "                            )\n",
    "\n",
    "                            if url.endswith(\".docx\"):\n",
    "                                with open(doc_path, \"wb\") as f:\n",
    "                                    f.write(response.content)\n",
    "                                subprocess.run(\n",
    "                                    [\n",
    "                                        soffice_path,\n",
    "                                        \"--convert-to\",\n",
    "                                        \"pdf\",\n",
    "                                        \"--headless\",\n",
    "                                        doc_path,\n",
    "                                    ]\n",
    "                                )\n",
    "                                os.rename(\"temp.pdf\", pdf_path)\n",
    "                                os.remove(\"temp.docx\")\n",
    "\n",
    "                            else:\n",
    "                                with open(pdf_path, \"wb\") as f:\n",
    "                                    f.write(response.content)\n",
    "\n",
    "                            # ADD PDF SCRAPER HERE\n",
    "                            pdfFileObj = open(pdf_path, \"rb\")\n",
    "                            pdfReaded = PyPDF2.PdfReader(pdfFileObj)\n",
    "\n",
    "                            # Get the number of pages in the PDF file\n",
    "                            num_pages = len(pdfReaded.pages)\n",
    "\n",
    "                            # Create the dictionary to extract text from each image\n",
    "                            text_per_page = {}\n",
    "\n",
    "                            # We extract the pages from the PDF\n",
    "                            for pagenum, page in enumerate(extract_pages(pdf_path)):\n",
    "                                if pagenum > 2:\n",
    "                                    break\n",
    "                                # Initialize the variables needed for the text extraction from the page\n",
    "                                pageObj = pdfReaded.pages[pagenum]\n",
    "                                page_text = []\n",
    "                                line_format = []\n",
    "                                page_content = []\n",
    "\n",
    "                                # Open the pdf file\n",
    "                                pdf = pdfplumber.open(pdf_path)\n",
    "\n",
    "                                # Find the examined page\n",
    "                                page_tables = pdf.pages[pagenum]\n",
    "\n",
    "                                # Find all the elements\n",
    "                                page_elements = [\n",
    "                                    (element.y1, element) for element in page._objs\n",
    "                                ]\n",
    "\n",
    "                                # Sort all the elements as they appear in the page\n",
    "                                page_elements.sort(key=lambda a: a[0], reverse=True)\n",
    "\n",
    "                                # Find the elements that composed a page\n",
    "                                for i, component in enumerate(page_elements):\n",
    "                                    # Extract the position of the top side of the element in the PDF\n",
    "                                    pos = component[0]\n",
    "\n",
    "                                    # Extract the element of the page layout\n",
    "                                    element = component[1]\n",
    "\n",
    "                                    # Check if the element is a text element\n",
    "                                    if isinstance(element, LTTextContainer):\n",
    "                                        # Use the function to extract the text and format for each text element\n",
    "                                        (line_text, format_per_line) = text_extraction(\n",
    "                                            element\n",
    "                                        )\n",
    "\n",
    "                                        # Append the text of each line to the page text\n",
    "                                        page_text.append(line_text)\n",
    "\n",
    "                                        # Append the format for each line containing text\n",
    "                                        line_format.append(format_per_line)\n",
    "                                        page_content.append(line_text)\n",
    "                                    # Create the key of the dictionary\n",
    "                                    dctkey = \"Page_\" + str(pagenum)\n",
    "\n",
    "                                    # Add the list of list as the value of the page key\n",
    "                                    text_per_page[dctkey] = [\n",
    "                                        page_text,\n",
    "                                        line_format,\n",
    "                                        page_content,\n",
    "                                    ]\n",
    "\n",
    "                                # Display the content of the page\n",
    "                                page_result = \"\".join(\n",
    "                                    text_per_page[\"Page_\" + str(pagenum)][0]\n",
    "                                )\n",
    "                                result = result + \"\\n \\n\" + page_result\n",
    "\n",
    "                                # Close the pdf file\n",
    "                                pdfFileObj.close()\n",
    "\n",
    "                                # Remove pdf files that are not needed anymore\n",
    "                            try:\n",
    "                                os.remove(pdf_path)\n",
    "                                files = glob(f\"*.pdf\")\n",
    "                                for f in files:\n",
    "                                    os.remove(f)\n",
    "                            except:\n",
    "                                print(\"No files to remove\")\n",
    "\n",
    "                            num = num + 1\n",
    "\n",
    "                            # Save the extracted text to the json file from the api call\n",
    "                            comment[\"data\"][\"attributes\"][\"pdf_extracted_text\"] = result\n",
    "                            comment[\"data\"][\"attributes\"][\n",
    "                                \"attachment_read\"\n",
    "                            ] = \"attachment extracted\"\n",
    "                            comment[\"data\"][\"attributes\"][\"attachments_url\"] = attachment_url\n",
    "\n",
    "                        except Exception as inst:\n",
    "                            print(type(inst))  # the exception type\n",
    "                            x = inst.args  # unpack args\n",
    "                            print(\"x =\", x)\n",
    "                            comment[\"data\"][\"attributes\"][\n",
    "                                \"attachment_read\"\n",
    "                            ] = \"attachment failed\"\n",
    "                            comment[\"data\"][\"attributes\"][\"attachments_url\"] = attachment_url\n",
    "                        except:\n",
    "                            comment[\"data\"][\"attributes\"][\n",
    "                                \"attachment_read\"\n",
    "                            ] = \"attachment failed\"\n",
    "                            comment[\"data\"][\"attributes\"][\"attachments_url\"] = attachment_url\n",
    "                            raise\n",
    "                else:\n",
    "                    comment[\"data\"][\"attributes\"][\"attachment_read\"] = \"no attachment\"\n",
    "                    comment[\"data\"][\"attributes\"][\"attachments_url\"] = None\n",
    "        except KeyError:\n",
    "            comment[\"data\"][\"attributes\"][\"attachment_read\"] = \"no attachment\"\n",
    "            comment[\"data\"][\"attributes\"][\"attachments_url\"] = None\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the pdf extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = get_comment_text(comment_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Reshape and save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_data(data, keys_to_include):\n",
    "    final_data = []\n",
    "    for comment in data:\n",
    "        # Flatten the nested dictionaries\n",
    "        flat_comment = flatten(comment)\n",
    "        result_dict = {}\n",
    "        for key, value in flat_comment.items():\n",
    "            if key in keys_to_include:\n",
    "                if isinstance(value, dict):\n",
    "                    # Recursively process nested dictionaries\n",
    "                    result_dict[key] = structure_data(value, keys_to_include)\n",
    "                else:\n",
    "                    # Include non-dictionary values\n",
    "                    result_dict[key] = value if value is not None else \"\"\n",
    "        final_data.append(result_dict)\n",
    "\n",
    "    # Rename the keys to match the database\n",
    "    key_mapping = {\n",
    "        \"data_id\": \"comment_id\",\n",
    "        \"data_attributes_commentOnDocumentId\": \"document_id\",\n",
    "        \"data_attributes_docketId\": \"docket_id\",\n",
    "        \"data_attributes_agencyId\": \"agency_id\",\n",
    "        \"data_attributes_title\": \"title\",\n",
    "        \"data_attributes_comment\": \"comment\",\n",
    "        \"data_attributes_pdf_extracted_text\": \"comment_pdf_extracted\",\n",
    "        \"data_attributes_firstName\": \"commenter_first_name\",\n",
    "        \"data_attributes_lastName\": \"commenter_last_name\",\n",
    "        \"data_attributes_organization\": \"commenter_organization\",\n",
    "        \"data_attributes_address1\": \"commenter_address1\",\n",
    "        \"data_attributes_address2\": \"commenter_address2\",\n",
    "        \"data_attributes_zip\": \"commenter_zip\",\n",
    "        \"data_attributes_city\": \"commenter_city\",\n",
    "        \"data_attributes_stateProvinceRegion\": \"commenter_state_province_region\",\n",
    "        \"data_attributes_country\": \"commenter_country\",\n",
    "        \"data_attributes_email\": \"commenter_email\",\n",
    "        \"data_attributes_receiveDate\": \"receive_date\",\n",
    "        \"data_attributes_postedDate\": \"posted_date\",\n",
    "        \"data_attributes_postmarkDate\": \"postmark_date\",\n",
    "        \"data_attributes_duplicateComments\": \"duplicate_comments\",\n",
    "        \"data_attributes_attachment_read\": \"attachment_read\",\n",
    "        \"data_attributes_attachments_url\": \"attachment_url\",\n",
    "        \"data_attributes_withdrawn\": \"withdrawn\",\n",
    "        \"data_links_self\": \"api_url\",\n",
    "    }\n",
    "\n",
    "    # Rename the keys so they match the database\n",
    "    for i in final_data:\n",
    "        for old_key, new_key in key_mapping.items():\n",
    "            i[new_key] = i.pop(old_key, \"\")\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the keys to include in the resulting dictionary\n",
    "keys_to_include = [\n",
    "    \"data_id\",\n",
    "    \"data_attributes_commentOnDocumentId\",\n",
    "    \"data_attributes_docketId\",\n",
    "    \"data_attributes_agencyId\",\n",
    "    \"data_attributes_title\",\n",
    "    \"data_attributes_comment\",\n",
    "    \"data_attributes_pdf_extracted_text\",\n",
    "    \"data_attributes_firstName\",\n",
    "    \"data_attributes_lastName\",\n",
    "    \"data_attributes_organization\",\n",
    "    \"data_attributes_address1\",\n",
    "    \"data_attributes_address2\",\n",
    "    \"data_attributes_zip\",\n",
    "    \"data_attributes_city\",\n",
    "    \"data_attributes_country\",\n",
    "    \"data_attributes_stateProvinceRegion\",\n",
    "    \"data_attributes_email\",\n",
    "    \"data_attributes_receiveDate\",\n",
    "    \"data_attributes_postedDate\",\n",
    "    \"data_attributes_postmarkDate\",\n",
    "    \"data_links_self\",\n",
    "    \"data_attributes_attachments_url\",\n",
    "    \"data_attributes_attachment_read\",\n",
    "    \"data_attributes_duplicateComments\",\n",
    "    \"data_attributes_withdrawn\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the nested JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = structure_data(full_data, keys_to_include)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(input_string):\n",
    "    \"\"\"Remove NULL characters from a string.\"\"\"\n",
    "    if input_string is not None:\n",
    "        # clean up html - list all the html characters that need to be changed and what they should be changed to\n",
    "        html_chars = {\n",
    "            \"&amp;\": \"&\",\n",
    "            \"&gt;\": \">\",\n",
    "            \"&lt;\": \"<\",\n",
    "            \"&nbsp;\": \" \",\n",
    "            \"&quot;\": '\"',\n",
    "            \"&#39;\": \"'\",\n",
    "            \"&#34;\": '\"',\n",
    "            \"nan\": \"\",\n",
    "            \"<br>\": \" \",\n",
    "            \"<br/>\": \" \",\n",
    "            \"\\n\": \" \",\n",
    "            \"\\x00\": \"\",\n",
    "        }\n",
    "        for key, value in html_chars.items():\n",
    "            input_string = input_string.replace(key, value)\n",
    "\n",
    "        input_string = input_string.replace(\"See Attached\", \"\")\n",
    "        input_string = input_string.replace(\"See attached file(s)\", \"\")\n",
    "        input_string = html.unescape(input_string)\n",
    "\n",
    "    return input_string\n",
    "\n",
    "\n",
    "# CREATE THE FULL TEXT AND CLEAN_TEXT COLUMNS:\n",
    "for item in result:\n",
    "    # Create a new column that combines the comment and the extracted text from the pdf\n",
    "    item[\"full_text\"] = item[\"comment\"] + \" \" + item[\"comment_pdf_extracted\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Structure the data into a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"{docket_id}_comments.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
