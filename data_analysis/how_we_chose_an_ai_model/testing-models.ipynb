{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing models\n",
    "\n",
    "To find the best model for our purpose, we decided to test a handful of the most common models on the market. \n",
    "\n",
    "We are testing these six different models from Anthropic, OpenAI and Google. \n",
    "\n",
    "- `claude-3-opus-20240229`\n",
    "- `claude-3-sonnet-20240229`\n",
    "- `claude-3-haiku-20240307`\n",
    "- `gpt-3.5-turbo-1106`\n",
    "- `gpt-4-1106-preview`\n",
    "- `gemini-pro`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load in the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2\n",
    "import requests\n",
    "\n",
    "import anthropic\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import google.generativeai as genai\n",
    "import rich\n",
    "\n",
    "from langchain.evaluation import JsonValidityEvaluator\n",
    "from pydantic import BaseModel, Field\n",
    "from guardrails import Guard\n",
    "from guardrails.hub import ValidLength\n",
    "from rich import print\n",
    "import guardrails as gd\n",
    "from typing import Optional\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Connect to the database and pull data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_password = os.getenv(\"COMMONS_DB_PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\n",
    "    f\"postgresql://doadmin:{db_password}@commons-database-do-user-15654205-0.c.db.ondigitalocean.com:25060/commons?sslmode=require\"\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a random sample of 200 comments from the database for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\n",
    "    f\"\"\"SELECT comment_id, docket_id, agency_id, comment, comment_pdf_extracted, commenter_first_name, commenter_last_name, commenter_organization, commenter_address1, commenter_address2, commenter_zip, commenter_city, commenter_state_province_region, commenter_country, commenter_email, receive_date, posted_date, postmark_date, api_url, attachment_read, attachment_url, duplicate_comments, withdrawn, title, full_text from COMMENTS\n",
    "               ORDER BY RANDOM()\n",
    "               LIMIT 200;\"\"\"\n",
    ")\n",
    "colnames = [desc[0] for desc in cursor.description]\n",
    "result = cursor.fetchall()\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And turn it into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(result, columns=colnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the models\n",
    "\n",
    "But first we need to load the api keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.organization = None\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "gemini_key = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create three columns to keep check of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ai_process_gpt-3.5-turbo-1106'] = False\n",
    "df['ai_process_gpt-4-1106-preview'] = False\n",
    "df['ai_process_claude-3-opus-20240229'] = False\n",
    "df['ai_process_claude-3-sonnet-20240229'] = False\n",
    "df['ai_process_claude-3-haiku-20240307'] = False\n",
    "df['ai_process_gemini-1.0-pro'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the json output template that guardrails will use to check the output of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = ValidLength(min=5, on_fail=\"reask\")\n",
    "\n",
    "class Comment(BaseModel):\n",
    "    ai_first_name: Optional[str] = (Field(default=None, description=\"The commenter's first name\"))\n",
    "    ai_middle_name: Optional[str] = (Field(default=None, description=\"The commenter's middle name or initial\"))\n",
    "    ai_last_name: Optional[str] = (Field(default=None, description=\"The commenter's last name\"))\n",
    "    ai_email: Optional[str] = (Field(default=None, description=\"The commenter's email address, if mentioned\"))\n",
    "    ai_phone: Optional[str] = (Field(default=None, description=\"The commenter's phone number, if mentioned\"))\n",
    "    ai_address: Optional[str] = (Field(default=None, description=\"Mailing address of the commenter\"))\n",
    "    ai_city: Optional[str] = (Field(default=None, description=\"City of the commenter\"))\n",
    "    ai_state: Optional[str] = (Field(default=None, description=\"State of the commenter e.g. MA for Massachusetts, MD for Maryland or PA for Pennsylvania.\"))\n",
    "    ai_zip: Optional[str] = (Field(default=None, description=\"Zipcode of the commenter\"))\n",
    "    ai_country: Optional[str] = (Field(default=None, description=\"Country of the commenter\"))\n",
    "    ai_job_title: Optional[str] = (Field(default=None, description=\"Job title of the commenter\"))\n",
    "    ai_org: Optional[str] = (Field(default=None, description=\"Organization of the commenter\"))\n",
    "    ai_summary: str = Field(description=\"A short two sentence summary of main points of the comment\",\n",
    "                            validators=[val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt: str = \"\"\"\n",
    "Read the following public comment on a federal regulation and extract ALL of the relevant information, \n",
    "including the commenter's first name, middle name, last name, email adress, phone number, address, city, \n",
    "state, zipcode, country, job title, affiliated organization, and a short 2 sentences long summary of what the \n",
    "commenter is saying. The summary cannot be empty or the string 'None'. If any of the information can't be derived, \n",
    "you must return null and nothing else.\n",
    "\n",
    "Example output format 1: \n",
    "'{\"ai_first_name\": \"Mary\",\n",
    "\"ai_middle_name\": \"C.\",\n",
    "\"ai_last_name\": \"Smith\",\n",
    "\"ai_email\": \"msmith@gmail.com\", \n",
    "\"ai_phone\": \"913-593-4889\",\n",
    "\"ai_address\": \"1604 Grand Ave.\",\n",
    "\"ai_city\": \"St. Paul\",\n",
    "\"ai_state\": \"MN\",\n",
    "\"ai_zip\": \"55105\",\n",
    "\"ai_country\": \"United States\",\n",
    "\"ai_job_title\": \"High school teacher\",\n",
    "\"ai_org\": \"Seattle Public Schools\",\n",
    "\"ai_summary\": \"The proposed regulation will help protect the environment and help keep the air clean for her five children. The administration must make an effort to pass this as soon as possible.\"}'\n",
    "\n",
    "Example output format 2:\n",
    "'{\"ai_first_name\": \"David\",\n",
    "\"ai_middle_name\": \"James\",\n",
    "\"ai_last_name\": \"Roberts\",\n",
    "\"ai_email\": \"david_roberts@hotmail.com\", \n",
    "\"ai_phone\": null,\n",
    "\"ai_address\": null,\n",
    "\"ai_city\": \"Sioux Falls\",\n",
    "\"ai_state\": \"SD\",\n",
    "\"ai_zip\": null,\n",
    "\"ai_country\": \"United States\",\n",
    "\"ai_job_title\": \"Mechanic\",\n",
    "\"ai_org\": null,\n",
    "\"ai_summary\": \"This regulation will hurt small businesses and make it harder for people to get to work. The administration should not pass this regulation.\"}'\n",
    "\n",
    "Example output format 3:\n",
    "'{\"ai_first_name\": \"Hanna\",\n",
    "\"ai_middle_name\": null,\n",
    "\"ai_last_name\": \"Chen\",\n",
    "\"ai_email\": \"bdfarms@aol.com\", \n",
    "\"ai_phone\": \"785-551-2009\",\n",
    "\"ai_address\": \"490 Del Matro Ave.\",\n",
    "\"ai_city\": \"Windsor Heights\",\n",
    "\"ai_state\": \"IA\",\n",
    "\"ai_zip\": \"50324\",\n",
    "\"ai_country\": \"United States\",\n",
    "\"ai_job_title\": \"Organizer\",\n",
    "\"ai_org\": \"Natural Resources Defense Council\",\n",
    "\"ai_summary\": \"The commenter expresses their love for the state and the environment. They say their family will continue to live in the state.\"}'\n",
    "\n",
    "Do not use the examples provided in the prompt to fill in the fields.\n",
    "\n",
    "Comment: ${comment}\n",
    "\n",
    "${gr.complete_json_suffix_v2}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anthropic Model `Claude-3`\n",
    "\n",
    "Here we are testing three versions of Anthropic's new model `Claude-3`. They are:\n",
    "\n",
    "- `Claude 3 Opus`: the most intelligent and capable model.\n",
    "\n",
    "- `Claude 3 Sonnet`: strikes the balance between intelligence and speedâ€”particularly for high-volume tasks.\n",
    "\n",
    "- `Claude 3 Haiku`: the fastest and most affordable model.\n",
    "\n",
    "(source: Email Laura got from Anthropic on March 17th, 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anthropic_models = [\"claude-3-opus-20240229\", \"claude-3-sonnet-20240229\", \"claude-3-haiku-20240307\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = anthropic.Anthropic(api_key=anthropic_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_claude_request(prompt: str, max_tokens: int, engine: str, **kwargs) -> str:\n",
    "    \"\"\"Claude LLM API wrapper.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt to be passed to the LLM API\n",
    "        engine (str): The name of the model to be used\n",
    "        max_tokens (int): The token limit to be passed to the LLM API\n",
    "        **kwargs: Any additional arguments to be passed to the LLM API\n",
    "\n",
    "    Returns:\n",
    "        str: The output of the LLM API\n",
    "    \"\"\"\n",
    "\n",
    "    message = client.messages.create(\n",
    "        max_tokens=max_tokens,\n",
    "        model=engine,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_anthropic(comment: str, claude_model: str ):\n",
    "    \n",
    "    guard = Guard.from_pydantic(output_class=Comment, prompt=system_prompt)\n",
    "\n",
    "    try:\n",
    "        raw_llm_output, validated_output, *rest = guard(\n",
    "            llm_api=make_claude_request,\n",
    "            engine= claude_model,\n",
    "            prompt_params={\"comment\": comment},\n",
    "            max_tokens=1024,\n",
    "            temperature=0\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        rich.print(guard.history.last.tree)\n",
    "        raise e\n",
    "\n",
    "    return validated_output\n",
    "\n",
    "# run it on the df column full text and save the results in a new column\n",
    "\n",
    "for claude_model in anthropic_models:\n",
    "    df[f'ai_response_{claude_model}'] = df['full_text'].progress_apply(lambda x: ask_anthropic(comment=x, claude_model=claude_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI Model `GPT-3.5` and `GPT-4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_models = [\"gpt-3.5-turbo-1106\", \"gpt-4-1106-preview\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that asks GPT for the information\n",
    "def ask_gpt(prompt, comment, gpt_model):\n",
    "    guard = Guard.from_pydantic(output_class=Comment, prompt=prompt + comment)\n",
    "\n",
    "    raw_llm_output, validated_output, *rest = guard(\n",
    "        llm_api = openai.chat.completions.create,\n",
    "            model=gpt_model,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "    evaluator = JsonValidityEvaluator()\n",
    "    output = validated_output\n",
    "    prediction = json.dumps(output)\n",
    "    result = evaluator.evaluate_strings(prediction=prediction)\n",
    "\n",
    "    if result['score'] == 1:\n",
    "        prediction = json.loads(prediction)\n",
    "        df[f'ai_process_{gpt_model}'] = True\n",
    "        return prediction\n",
    "    else:\n",
    "        print(\"Generated response is not valid JSON.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gpt_model in gpt_models:\n",
    "    def process_row(row):\n",
    "        if not row[f'ai_process_{gpt_model}']:\n",
    "            row[f'ai_response_{gpt_model}'] = ask_gpt(system_prompt, row['full_text'], gpt_model)\n",
    "            row[f'ai_process_{gpt_model}'] = True  # Assuming ask_gpt sets ai_process to True\n",
    "        return row\n",
    "\n",
    "    rounds = 0\n",
    "\n",
    "    while not df[f'ai_process_{gpt_model}'].all() and rounds < 10:\n",
    "        df = df.apply(process_row, axis=1)\n",
    "        rounds += 1\n",
    "        print(f\"Round {rounds} complete\")\n",
    "    \n",
    "    print(F\"Done with {gpt_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google Model `Gemini`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key = gemini_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_settings = [\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "    \"threshold\": \"BLOCK_NONE\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "    \"threshold\": \"BLOCK_NONE\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "    \"threshold\": \"BLOCK_NONE\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "    \"threshold\": \"BLOCK_NONE\"\n",
    "  },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gemini_request(system_prompt: str, max_tokens: int = 2048, temperature: float = 0, **kwargs) -> str:\n",
    "    \"\"\"Gemini LLM API wrapper.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt to be passed to the LLM API\n",
    "        **kwargs: Any additional arguments to be passed to the LLM API\n",
    "\n",
    "    Returns:\n",
    "        str: The output of the LLM API\n",
    "    \"\"\"\n",
    "    gen_config = genai.types.GenerationConfig(\n",
    "        max_output_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        **kwargs)\n",
    "    \n",
    "    model = genai.GenerativeModel('gemini-pro', generation_config=gen_config, safety_settings=safety_settings)\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            system_prompt,\n",
    "            generation_config=gen_config\n",
    "        )\n",
    "\n",
    "        print(response)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(response.prompt_feedback)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_gemini(comment):\n",
    "    guard = Guard.from_pydantic(output_class=Comment, prompt=system_prompt)\n",
    "    raw_llm_output, validated_output, *rest = guard(\n",
    "        llm_api=make_gemini_request,\n",
    "        prompt_params={\"comment\": comment},\n",
    "        max_tokens=2048,\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    return validated_output   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ai_response_gemini-1.0-pro'] = df['full_text'].progress_apply(lambda x: ask_gemini(comment=x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ai_response(response, key):\n",
    "    try:\n",
    "        # Replace all strings with null with None\n",
    "        response = {k: None if v == \"null\" else v for k, v in response.items()}\n",
    "        # Replace all True with None\n",
    "        response = {k: None if v == True else v for k, v in response.items()}\n",
    "        return response[key]\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"claude-3-opus-20240229\", \"claude-3-sonnet-20240229\", \"claude-3-haiku-20240307\", \"gpt-3.5-turbo-1106\", \"gpt-4-1106-preview\", \"gemini-1.0-pro\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "\n",
    "    ai_response_list = df[f'ai_response_{model}'].tolist()\n",
    "\n",
    "    #keys = df[f\"ai_response_{gpt_model}\"].iloc[0].keys()\n",
    "    keys = ['ai_first_name', 'ai_middle_name', 'ai_last_name', 'ai_email', \n",
    "            'ai_phone', 'ai_address', 'ai_city', 'ai_state', 'ai_zip', \n",
    "            'ai_country', 'ai_job_title', 'ai_org', 'ai_summary']\n",
    "    \n",
    "    for key in keys:\n",
    "        df[f\"{key}_{model}\"] = df[f\"ai_response_{model}\"].progress_apply(lambda x: extract_ai_response(x, key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
